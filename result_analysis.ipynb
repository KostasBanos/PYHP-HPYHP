{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESULT ANALYSIS FOR PITMAN-YOR HAWKES PROCESS\n",
    "# =============================================================================\n",
    "# This notebook analyzes the results from the Sequential Monte Carlo sampling\n",
    "# of the Pitman-Yor Hawkes Process model. It examines cluster assignments,\n",
    "# word distributions, and topic coherence across different clusters.\n",
    "\n",
    "# Import required libraries for data analysis\n",
    "import pickle  # For loading serialized particle data\n",
    "import json    # For loading vocabulary mappings\n",
    "import numpy as np  # For numerical operations\n",
    "from collections import Counter  # For counting cluster assignments\n",
    "from utils import * # Custom utility functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD PARTICLE DATA\n",
    "# =============================================================================\n",
    "# Load the particles from the SMC sampling results. Each particle contains:\n",
    "# - Cluster assignments for all documents\n",
    "# - Word distributions for each cluster\n",
    "# - Temporal parameters and triggering kernels\n",
    "# - Log probabilities and weights\n",
    "\n",
    "with open('results/particles.pkl','rb') as f:\n",
    "    particles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD VOCABULARY MAPPING\n",
    "# =============================================================================\n",
    "# Load the word-to-ID mapping dictionary that was created during preprocessing.\n",
    "# This allows us to convert word IDs back to actual words for analysis.\n",
    "\n",
    "with open('data/id2word.json','r') as f:\n",
    "    id2word = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SELECT REPRESENTATIVE PARTICLE\n",
    "# =============================================================================\n",
    "# Since all particles have the same weight (verified), we can select any particle\n",
    "# for analysis. We'll use the first particle to examine cluster assignments\n",
    "# and word distributions.\n",
    "\n",
    "# have verified that all the weights are the same\n",
    "particle = particles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE CLUSTER ASSIGNMENTS\n",
    "# =============================================================================\n",
    "# Count how many documents are assigned to each cluster to understand\n",
    "# the clustering structure and identify dominant topics.\n",
    "\n",
    "cls_cnt = Counter(particle.docs2cluster_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document number: 48986\n",
      "cluster number: 1013\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DISPLAY CLUSTERING STATISTICS\n",
    "# =============================================================================\n",
    "# Print basic statistics about the clustering results:\n",
    "# - Total number of documents processed\n",
    "# - Total number of clusters discovered\n",
    "\n",
    "print('document number: %d' % len(particle.docs2cluster_ID) )\n",
    "print('cluster number: %d' % len(cls_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 clusters:\n",
      "cluster 2: 23914 documents\n",
      "cluster 5: 7997 documents\n",
      "cluster 20: 4472 documents\n",
      "cluster 10: 3231 documents\n",
      "cluster 544: 2462 documents\n",
      "cluster 493: 1488 documents\n",
      "cluster 852: 1223 documents\n",
      "cluster 699: 1174 documents\n",
      "cluster 466: 810 documents\n",
      "cluster 899: 113 documents\n",
      "cluster 357: 99 documents\n",
      "cluster 937: 88 documents\n",
      "cluster 435: 55 documents\n",
      "cluster 922: 53 documents\n",
      "cluster 194: 49 documents\n",
      "cluster 500: 43 documents\n",
      "cluster 955: 34 documents\n",
      "cluster 3: 29 documents\n",
      "cluster 340: 28 documents\n",
      "cluster 536: 25 documents\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TOP CLUSTERS BY DOCUMENT COUNT\n",
    "# =============================================================================\n",
    "# Display the top 10 clusters ranked by number of documents assigned to them.\n",
    "# This helps identify the most prominent topics discovered by the model.\n",
    "\n",
    "print('top 10 clusters:')\n",
    "for cls_id, count in cls_cnt.most_common(20):\n",
    "    print('cluster %d: %d documents' %( cls_id, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "white 23542.0\n",
      "republican 23463.0\n",
      "democrat 21869.0\n",
      "administr 20905.0\n",
      "obama 20631.0\n",
      "elect 20427.0\n",
      "compani 20070.0\n",
      "polici 17203.0\n",
      "plan 17004.0\n",
      "parti 16834.0\n",
      "vote 16616.0\n",
      "senat 16555.0\n",
      "law 15976.0\n",
      "percent 15424.0\n",
      "women 14960.0\n",
      "million 14369.0\n",
      "washington 14188.0\n",
      "power 13720.0\n",
      "media 13674.0\n",
      "busi 13668.0\n",
      "job 13636.0\n",
      "chang 13380.0\n",
      "america 13172.0\n",
      "leader 12962.0\n",
      "secur 12883.0\n",
      "feder 12508.0\n",
      "order 12400.0\n",
      "court 12109.0\n",
      "school 12007.0\n",
      "execut 11856.0\n",
      "member 11665.0\n",
      "citi 11558.0\n",
      "post 11520.0\n",
      "talk 11514.0\n",
      "immigr 11345.0\n",
      "famili 11156.0\n",
      "york 11086.0\n",
      "critic 11033.0\n",
      "meet 10947.0\n",
      "deal 10932.0\n",
      "russia 10823.0\n",
      "clinton 10766.0\n",
      "stori 10696.0\n",
      "interest 10687.0\n",
      "major 10636.0\n",
      "rule 10605.0\n",
      "case 10590.0\n",
      "tax 10558.0\n",
      "organ 10489.0\n",
      "commun 10462.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 2 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 2, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 2 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[2]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  2\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 2 # cluster 2\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polic 14511.0\n",
      "attack 10051.0\n",
      "court 9385.0\n",
      "immigr 9219.0\n",
      "order 8599.0\n",
      "citi 8350.0\n",
      "kill 7601.0\n",
      "law 7216.0\n",
      "case 7024.0\n",
      "secur 6621.0\n",
      "forc 6425.0\n",
      "famili 6117.0\n",
      "depart 5952.0\n",
      "feder 5791.0\n",
      "investig 5341.0\n",
      "execut 5169.0\n",
      "author 4987.0\n",
      "arrest 4982.0\n",
      "ban 4981.0\n",
      "charg 4961.0\n",
      "administr 4829.0\n",
      "judg 4805.0\n",
      "man 4797.0\n",
      "islam 4685.0\n",
      "statement 4466.0\n",
      "member 4388.0\n",
      "home 4287.0\n",
      "refuge 4281.0\n",
      "travel 4239.0\n",
      "crime 4214.0\n",
      "death 4213.0\n",
      "enforc 4179.0\n",
      "border 4039.0\n",
      "syria 4013.0\n",
      "found 3921.0\n",
      "militari 3838.0\n",
      "fire 3816.0\n",
      "prison 3806.0\n",
      "commun 3788.0\n",
      "muslim 3771.0\n",
      "children 3750.0\n",
      "justic 3685.0\n",
      "attorney 3596.0\n",
      "claim 3554.0\n",
      "victim 3540.0\n",
      "school 3519.0\n",
      "crimin 3367.0\n",
      "friday 3323.0\n",
      "area 3298.0\n",
      "face 3284.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 5 (TECHNOLOGY)\n",
    "# =============================================================================\n",
    "# Analyze cluster 5 which appears to contain technology-related articles.\n",
    "# Top words include: app, wine, car, hotel, smart, data, company, etc.\n",
    "# This suggests a mix of technology and lifestyle content.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 5 (appears to be technology/lifestyle related)\n",
    "specified_cls = particle.clusters[5]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  5\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 5 # cluster 5\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studi 5816.0\n",
      "research 5761.0\n",
      "health 5251.0\n",
      "drug 4035.0\n",
      "found 3842.0\n",
      "food 3627.0\n",
      "life 3576.0\n",
      "univers 3496.0\n",
      "human 3463.0\n",
      "famili 3380.0\n",
      "patient 3375.0\n",
      "children 3349.0\n",
      "percent 3346.0\n",
      "school 3291.0\n",
      "find 3251.0\n",
      "home 3191.0\n",
      "women 3153.0\n",
      "care 3143.0\n",
      "medic 3134.0\n",
      "water 2931.0\n",
      "compani 2928.0\n",
      "chang 2798.0\n",
      "diseas 2771.0\n",
      "doctor 2752.0\n",
      "feel 2611.0\n",
      "lot 2571.0\n",
      "scienc 2555.0\n",
      "scientist 2488.0\n",
      "problem 2428.0\n",
      "citi 2370.0\n",
      "develop 2329.0\n",
      "system 2304.0\n",
      "test 2291.0\n",
      "case 2281.0\n",
      "million 2272.0\n",
      "effect 2253.0\n",
      "im 2253.0\n",
      "place 2217.0\n",
      "parent 2208.0\n",
      "program 2207.0\n",
      "didnt 2188.0\n",
      "dr 2174.0\n",
      "high 2074.0\n",
      "theyr 2059.0\n",
      "death 2050.0\n",
      "brain 2045.0\n",
      "cancer 2028.0\n",
      "increas 2013.0\n",
      "hospit 2012.0\n",
      "risk 2011.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 20 (SPORTS)\n",
    "# =============================================================================\n",
    "# This appears to be a different analysis of cluster 20, showing sports-related content.\n",
    "# Top words include: playoff, week, team, packer, game, etc.\n",
    "# This suggests cluster 4 might contain both political and sports content,\n",
    "# or this might be analyzing a different cluster ID.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 20 (appears to be sports-related content)\n",
    "specified_cls = particle.clusters[20]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  20\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 20 # cluster 20\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 7912.0\n",
      "play 6329.0\n",
      "team 6293.0\n",
      "season 4909.0\n",
      "player 4866.0\n",
      "win 3136.0\n",
      "leagu 2786.0\n",
      "final 2530.0\n",
      "im 2484.0\n",
      "he 2454.0\n",
      "coach 2298.0\n",
      "fan 2259.0\n",
      "sport 2251.0\n",
      "run 2209.0\n",
      "open 1951.0\n",
      "hit 1842.0\n",
      "night 1824.0\n",
      "feel 1802.0\n",
      "didnt 1773.0\n",
      "met 1734.0\n",
      "guy 1729.0\n",
      "ball 1716.0\n",
      "big 1703.0\n",
      "great 1624.0\n",
      "yanke 1609.0\n",
      "lot 1599.0\n",
      "home 1592.0\n",
      "left 1503.0\n",
      "won 1502.0\n",
      "manag 1496.0\n",
      "man 1444.0\n",
      "set 1422.0\n",
      "score 1399.0\n",
      "record 1380.0\n",
      "million 1369.0\n",
      "turn 1367.0\n",
      "talk 1357.0\n",
      "bowl 1355.0\n",
      "lead 1355.0\n",
      "love 1353.0\n",
      "top 1347.0\n",
      "field 1324.0\n",
      "star 1322.0\n",
      "citi 1314.0\n",
      "career 1312.0\n",
      "major 1301.0\n",
      "sunday 1277.0\n",
      "club 1273.0\n",
      "pick 1271.0\n",
      "nfl 1268.0\n",
      "shot 1265.0\n",
      "music 1260.0\n",
      "footbal 1239.0\n",
      "head 1237.0\n",
      "super 1225.0\n",
      "perform 1213.0\n",
      "minut 1207.0\n",
      "goal 1206.0\n",
      "tournament 1167.0\n",
      "defens 1164.0\n",
      "watch 1144.0\n",
      "life 1124.0\n",
      "return 1123.0\n",
      "knick 1121.0\n",
      "round 1120.0\n",
      "chanc 1106.0\n",
      "titl 1103.0\n",
      "match 1099.0\n",
      "moment 1076.0\n",
      "miss 1068.0\n",
      "face 1067.0\n",
      "pitch 1052.0\n",
      "seri 1035.0\n",
      "espn 1035.0\n",
      "appear 1029.0\n",
      "ask 1019.0\n",
      "happen 1013.0\n",
      "basebal 1010.0\n",
      "past 1008.0\n",
      "place 1003.0\n",
      "line 1000.0\n",
      "side 991.0\n",
      "pass 991.0\n",
      "hope 979.0\n",
      "give 979.0\n",
      "take 979.0\n",
      "giant 978.0\n",
      "kind 977.0\n",
      "hard 965.0\n",
      "post 963.0\n",
      "ive 958.0\n",
      "champion 956.0\n",
      "saturday 953.0\n",
      "quarterback 953.0\n",
      "your 949.0\n",
      "finish 944.0\n",
      "find 938.0\n",
      "lost 937.0\n",
      "earli 934.0\n",
      "expect 932.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 10 (LIFESTYLE/ORGANIZATION)\n",
    "# =============================================================================\n",
    "# Analyze cluster 10 which appears to contain lifestyle and organization-related content.\n",
    "# Top words include: organ, drawer, toy, piece, find, cloth, etc.\n",
    "# This suggests content about home organization, lifestyle tips, and consumer products.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 28 (appears to be lifestyle/organization related)\n",
    "specified_cls = particle.clusters[10]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 100\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  10\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 10 # cluster 10\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compani 8591.0\n",
      "percent 5418.0\n",
      "billion 3682.0\n",
      "market 3336.0\n",
      "bank 3273.0\n",
      "million 2607.0\n",
      "busi 2527.0\n",
      "trade 2305.0\n",
      "share 2245.0\n",
      "uber 2088.0\n",
      "investor 2041.0\n",
      "invest 1989.0\n",
      "price 1960.0\n",
      "deal 1930.0\n",
      "industri 1842.0\n",
      "financi 1797.0\n",
      "firm 1745.0\n",
      "car 1716.0\n",
      "plan 1715.0\n",
      "sale 1691.0\n",
      "expect 1677.0\n",
      "servic 1676.0\n",
      "product 1617.0\n",
      "fund 1610.0\n",
      "rate 1586.0\n",
      "manag 1556.0\n",
      "stock 1540.0\n",
      "job 1508.0\n",
      "china 1486.0\n",
      "execut 1472.0\n",
      "tax 1454.0\n",
      "data 1441.0\n",
      "technolog 1439.0\n",
      "system 1392.0\n",
      "economi 1358.0\n",
      "growth 1317.0\n",
      "appl 1307.0\n",
      "wednesday 1305.0\n",
      "cut 1301.0\n",
      "global 1284.0\n",
      "secur 1277.0\n",
      "custom 1257.0\n",
      "sourc 1251.0\n",
      "increas 1238.0\n",
      "chief 1233.0\n",
      "consum 1205.0\n",
      "pay 1190.0\n",
      "declin 1189.0\n",
      "offer 1181.0\n",
      "worker 1178.0\n",
      "oper 1170.0\n",
      "feder 1162.0\n",
      "analyst 1150.0\n",
      "hold 1134.0\n",
      "tuesday 1126.0\n",
      "googl 1120.0\n",
      "amazon 1120.0\n",
      "edit 1112.0\n",
      "target 1111.0\n",
      "sign 1093.0\n",
      "major 1082.0\n",
      "cost 1070.0\n",
      "capit 1067.0\n",
      "retail 1066.0\n",
      "rule 1056.0\n",
      "sell 1049.0\n",
      "develop 1045.0\n",
      "store 1038.0\n",
      "econom 1035.0\n",
      "meet 1028.0\n",
      "close 1026.0\n",
      "money 1018.0\n",
      "buy 1017.0\n",
      "user 1001.0\n",
      "quarter 997.0\n",
      "comment 990.0\n",
      "internet 989.0\n",
      "big 971.0\n",
      "inflat 969.0\n",
      "tesla 969.0\n",
      "polici 968.0\n",
      "employe 959.0\n",
      "privat 950.0\n",
      "york 947.0\n",
      "chang 943.0\n",
      "regul 939.0\n",
      "interest 936.0\n",
      "reuter 922.0\n",
      "board 921.0\n",
      "profit 915.0\n",
      "provid 912.0\n",
      "ceo 912.0\n",
      "take 892.0\n",
      "high 892.0\n",
      "matter 891.0\n",
      "rais 891.0\n",
      "administr 871.0\n",
      "revenu 870.0\n",
      "number 865.0\n",
      "statement 863.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 544 (HEALTH/SCIENCE)\n",
    "# =============================================================================\n",
    "# Analyze cluster 544 which appears to contain health and science-related content.\n",
    "# Top words include: study, research, health, drug, patient, university, etc.\n",
    "# This cluster clearly focuses on medical research, healthcare, and scientific studies.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 544 (appears to be health/science related)\n",
    "specified_cls = particle.clusters[544]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 100\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  544\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 544 # cluster 544\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "republican 9005.0\n",
      "bill 7215.0\n",
      "health 6423.0\n",
      "care 5429.0\n",
      "democrat 4852.0\n",
      "plan 4600.0\n",
      "tax 4485.0\n",
      "insur 4439.0\n",
      "obamacar 4176.0\n",
      "vote 4009.0\n",
      "senat 3996.0\n",
      "ryan 2570.0\n",
      "medicaid 2338.0\n",
      "legisl 2335.0\n",
      "conserv 2268.0\n",
      "repeal 2216.0\n",
      "gop 2151.0\n",
      "pass 2115.0\n",
      "act 2098.0\n",
      "white 2041.0\n",
      "percent 2035.0\n",
      "coverag 1960.0\n",
      "member 1931.0\n",
      "parti 1929.0\n",
      "congress 1783.0\n",
      "budget 1770.0\n",
      "fund 1754.0\n",
      "major 1724.0\n",
      "law 1680.0\n",
      "cut 1666.0\n",
      "cost 1666.0\n",
      "leader 1644.0\n",
      "caucu 1619.0\n",
      "afford 1616.0\n",
      "replac 1520.0\n",
      "million 1510.0\n",
      "freedom 1506.0\n",
      "feder 1504.0\n",
      "administr 1503.0\n",
      "spend 1484.0\n",
      "polici 1385.0\n",
      "program 1332.0\n",
      "propos 1313.0\n",
      "pay 1260.0\n",
      "reform 1239.0\n",
      "premium 1223.0\n",
      "paul 1168.0\n",
      "congression 1151.0\n",
      "requir 1127.0\n",
      "elect 1108.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 493 (SPORTS)\n",
    "# =============================================================================\n",
    "# Analyze cluster 493 which appears to contain sports-related content.\n",
    "# Top words include: game, team, play, player, season, win, league, etc.\n",
    "# This cluster is clearly focused on sports coverage, including various leagues\n",
    "# and athletic competitions.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 493 (appears to be sports related)\n",
    "specified_cls = particle.clusters[493]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  493\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 493 # cluster 493\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comey 8083.0\n",
      "investig 6216.0\n",
      "fbi 4278.0\n",
      "russia 3359.0\n",
      "russian 3125.0\n",
      "director 2877.0\n",
      "fire 2805.0\n",
      "white 2748.0\n",
      "senat 2309.0\n",
      "intellig 2231.0\n",
      "committe 2102.0\n",
      "flynn 2075.0\n",
      "elect 1947.0\n",
      "attorney 1889.0\n",
      "justic 1804.0\n",
      "democrat 1714.0\n",
      "inform 1533.0\n",
      "depart 1462.0\n",
      "republican 1441.0\n",
      "session 1396.0\n",
      "clinton 1374.0\n",
      "secur 1366.0\n",
      "meet 1366.0\n",
      "mueller 1366.0\n",
      "special 1351.0\n",
      "administr 1301.0\n",
      "ask 1294.0\n",
      "jame 1282.0\n",
      "counsel 1252.0\n",
      "convers 1174.0\n",
      "advis 1040.0\n",
      "rosenstein 1035.0\n",
      "washington 1027.0\n",
      "probe 1005.0\n",
      "testimoni 989.0\n",
      "post 969.0\n",
      "decis 957.0\n",
      "statement 944.0\n",
      "press 923.0\n",
      "leak 916.0\n",
      "law 910.0\n",
      "email 862.0\n",
      "cnn 861.0\n",
      "hear 860.0\n",
      "congress 822.0\n",
      "\t 819.0\n",
      "sourc 815.0\n",
      "deputi 813.0\n",
      "stori 805.0\n",
      "tuesday 802.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 852 (HEALTHCARE POLICY)\n",
    "# =============================================================================\n",
    "# Analyze cluster 852 which appears to contain healthcare policy and political content.\n",
    "# Top words include: republican, bill, health, care, insurance, plan, tax, etc.\n",
    "# This cluster focuses on healthcare legislation, political debates around\n",
    "# healthcare reform, and policy discussions.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 852 (appears to be healthcare policy related)\n",
    "specified_cls = particle.clusters[852]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  852\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 852 # cluster 852\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north 5286.0\n",
      "korea 4845.0\n",
      "missil 2655.0\n",
      "militari 2456.0\n",
      "syria 2299.0\n",
      "nuclear 2079.0\n",
      "forc 2010.0\n",
      "china 1948.0\n",
      "attack 1877.0\n",
      "administr 1861.0\n",
      "weapon 1849.0\n",
      "korean 1823.0\n",
      "russia 1795.0\n",
      "war 1685.0\n",
      "strike 1555.0\n",
      "iran 1549.0\n",
      "south 1463.0\n",
      "assad 1433.0\n",
      "syrian 1385.0\n",
      "secur 1315.0\n",
      "leader 1302.0\n",
      "saudi 1289.0\n",
      "test 1199.0\n",
      "polici 1185.0\n",
      "chemic 1162.0\n",
      "foreign 1158.0\n",
      "washington 1132.0\n",
      "defens 1130.0\n",
      "russian 1125.0\n",
      "region 1115.0\n",
      "alli 1111.0\n",
      "qatar 1111.0\n",
      "tillerson 1110.0\n",
      "obama 1109.0\n",
      "regim 1043.0\n",
      "deal 1010.0\n",
      "white 975.0\n",
      "power 972.0\n",
      "meet 966.0\n",
      "action 950.0\n",
      "kim 943.0\n",
      "secretari 913.0\n",
      "launch 901.0\n",
      "statement 897.0\n",
      "islam 887.0\n",
      "intern 858.0\n",
      "pyongyang 853.0\n",
      "arabia 838.0\n",
      "sanction 788.0\n",
      "\t 765.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 699 (RUSSIA INVESTIGATION)\n",
    "# =============================================================================\n",
    "# Analyze cluster 699 which appears to contain content about the Russia investigation.\n",
    "# Top words include: comey, investigation, fbi, russia, russian, director, etc.\n",
    "# This cluster focuses on political investigations, FBI activities, and\n",
    "# related legal and political developments.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 669906 (appears to be Russia investigation related)\n",
    "specified_cls = particle.clusters[699]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  699\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 699 # cluster 699\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intellig 3212.0\n",
      "investig 3108.0\n",
      "russian 3006.0\n",
      "committe 2434.0\n",
      "russia 2251.0\n",
      "white 2119.0\n",
      "obama 1991.0\n",
      "nune 1881.0\n",
      "elect 1606.0\n",
      "inform 1506.0\n",
      "democrat 1481.0\n",
      "administr 1379.0\n",
      "session 1320.0\n",
      "claim 1221.0\n",
      "senat 1197.0\n",
      "wiretap 1159.0\n",
      "fbi 1158.0\n",
      "flynn 1155.0\n",
      "republican 1142.0\n",
      "surveil 1126.0\n",
      "secur 1106.0\n",
      "attorney 1078.0\n",
      "comey 1062.0\n",
      "evid 1025.0\n",
      "alleg 1021.0\n",
      "foreign 1000.0\n",
      "commun 980.0\n",
      "depart 950.0\n",
      "meet 950.0\n",
      "advis 874.0\n",
      "ask 860.0\n",
      "clinton 822.0\n",
      "agenc 817.0\n",
      "director 808.0\n",
      "member 797.0\n",
      "justic 797.0\n",
      "spicer 769.0\n",
      "press 759.0\n",
      "statement 727.0\n",
      "hear 723.0\n",
      "ambassador 715.0\n",
      "schiff 679.0\n",
      "tweet 677.0\n",
      "chairman 660.0\n",
      "contact 652.0\n",
      "presidenti 638.0\n",
      "confirm 633.0\n",
      "manafort 628.0\n",
      "washington 618.0\n",
      "sourc 614.0\n",
      "stori 614.0\n",
      "order 611.0\n",
      "accus 592.0\n",
      "post 590.0\n",
      "law 586.0\n",
      "email 580.0\n",
      "congress 574.0\n",
      "tie 559.0\n",
      "associ 549.0\n",
      "comment 545.0\n",
      "team 537.0\n",
      "document 529.0\n",
      "involv 523.0\n",
      "tower 516.0\n",
      "monday 516.0\n",
      "cnn 515.0\n",
      "media 513.0\n",
      "case 494.0\n",
      "leak 492.0\n",
      "york 484.0\n",
      "top 483.0\n",
      "request 477.0\n",
      "friday 474.0\n",
      "deni 473.0\n",
      "court 471.0\n",
      "march 471.0\n",
      "resign 468.0\n",
      "aid 464.0\n",
      "recus 458.0\n",
      "thursday 453.0\n",
      "transit 451.0\n",
      "discuss 445.0\n",
      "twitter 441.0\n",
      "suggest 434.0\n",
      "charg 432.0\n",
      "talk 431.0\n",
      "jame 425.0\n",
      "fact 425.0\n",
      "kislyak 425.0\n",
      "hack 424.0\n",
      "wednesday 423.0\n",
      "putin 417.0\n",
      "act 416.0\n",
      "interview 415.0\n",
      "senior 410.0\n",
      "provid 404.0\n",
      "rice 400.0\n",
      "met 400.0\n",
      "bharara 399.0\n",
      "feder 394.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 466 (NORTH KOREA)\n",
    "# =============================================================================\n",
    "# Analyze cluster 466 which appears to contain content about North Korea.\n",
    "# Top words include: north, korea, missile, china, korean, nuclear, etc.\n",
    "# This cluster focuses on international relations, military developments,\n",
    "# and geopolitical issues related to North Korea.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 466 (appears to be North Korea related)\n",
    "specified_cls = particle.clusters[466]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  466\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 466 # cluster 466\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manchest 756.0\n",
      "attack 577.0\n",
      "polic 496.0\n",
      "abedi 312.0\n",
      "concert 263.0\n",
      "bomb 236.0\n",
      "grand 204.0\n",
      "famili 201.0\n",
      "terror 188.0\n",
      "arena 184.0\n",
      "british 159.0\n",
      "secur 155.0\n",
      "monday 147.0\n",
      "victim 147.0\n",
      "investig 145.0\n",
      "night 144.0\n",
      "friend 141.0\n",
      "kill 140.0\n",
      "arrest 139.0\n",
      "children 138.0\n",
      "love 135.0\n",
      "statement 134.0\n",
      "citi 125.0\n",
      "tuesday 122.0\n",
      "ariana 122.0\n",
      "explos 121.0\n",
      "terrorist 116.0\n",
      "britain 115.0\n",
      "salman 106.0\n",
      "confirm 101.0\n",
      "threat 101.0\n",
      "london 99.0\n",
      "servic 97.0\n",
      "libya 95.0\n",
      "man 94.0\n",
      "minist 94.0\n",
      "daughter 91.0\n",
      "level 89.0\n",
      "bomber 85.0\n",
      "uk 83.0\n",
      "greater 83.0\n",
      "inform 82.0\n",
      "event 82.0\n",
      "respons 82.0\n",
      "carri 81.0\n",
      "suicid 80.0\n",
      "injur 79.0\n",
      "arm 78.0\n",
      "twitter 78.0\n",
      "prime 77.0\n",
      "intellig 77.0\n",
      "fan 76.0\n",
      "post 76.0\n",
      "media 75.0\n",
      "home 75.0\n",
      "place 74.0\n",
      "young 74.0\n",
      "brother 74.0\n",
      "wednesday 73.0\n",
      "author 72.0\n",
      "area 70.0\n",
      "critic 70.0\n",
      "share 69.0\n",
      "emerg 68.0\n",
      "claim 68.0\n",
      "england 67.0\n",
      "rais 67.0\n",
      "islam 66.0\n",
      "leak 66.0\n",
      "thought 66.0\n",
      "hospit 64.0\n",
      "remain 63.0\n",
      "cnn 63.0\n",
      "parent 61.0\n",
      "blast 61.0\n",
      "updat 61.0\n",
      "suspect 61.0\n",
      "die 61.0\n",
      "network 59.0\n",
      "girl 59.0\n",
      "releas 59.0\n",
      "singer 58.0\n",
      "plan 58.0\n",
      "mother 57.0\n",
      "return 57.0\n",
      "tweet 57.0\n",
      "school 56.0\n",
      "street 56.0\n",
      "act 55.0\n",
      "name 53.0\n",
      "crowd 53.0\n",
      "incid 53.0\n",
      "social 52.0\n",
      "identifi 52.0\n",
      "pop 51.0\n",
      "dead 51.0\n",
      "chief 51.0\n",
      "happen 51.0\n",
      "rudd 50.0\n",
      "member 50.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 899 (NORTH KOREA MALAYSIA INCIDENT)\n",
    "# =============================================================================\n",
    "# Analyze cluster 899 which appears to contain content about a specific North Korea incident.\n",
    "# Top words include: north, korea, korean, missile, kim, malaysia, etc.\n",
    "# This cluster focuses on a specific incident involving North Korea and Malaysia,\n",
    "# possibly related to the assassination of Kim Jong-nam.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 899 (appears to be North Korea-Malaysia incident related)\n",
    "specified_cls = particle.clusters[899]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 100\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  899\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 899 # cluster 899\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north 1115.0\n",
      "kim 835.0\n",
      "korea 736.0\n",
      "korean 588.0\n",
      "jong 391.0\n",
      "china 361.0\n",
      "missil 326.0\n",
      "malaysia 305.0\n",
      "malaysian 278.0\n",
      "south 266.0\n",
      "polic 255.0\n",
      "nam 236.0\n",
      "weapon 201.0\n",
      "kill 183.0\n",
      "nuclear 173.0\n",
      "vx 164.0\n",
      "airport 159.0\n",
      "lumpur 139.0\n",
      "kuala 139.0\n",
      "attack 138.0\n",
      "intern 138.0\n",
      "agent 136.0\n",
      "suspect 133.0\n",
      "chemic 133.0\n",
      "leader 132.0\n",
      "pyongyang 129.0\n",
      "test 129.0\n",
      "secur 128.0\n",
      "assassin 125.0\n",
      "death 125.0\n",
      "author 110.0\n",
      "women 103.0\n",
      "system 103.0\n",
      "foreign 99.0\n",
      "face 97.0\n",
      "beij 96.0\n",
      "bodi 94.0\n",
      "ballist 93.0\n",
      "militari 92.0\n",
      "nerv 91.0\n",
      "investig 90.0\n",
      "statement 90.0\n",
      "launch 89.0\n",
      "murder 87.0\n",
      "chines 87.0\n",
      "diplomat 84.0\n",
      "sanction 83.0\n",
      "ministri 83.0\n",
      "arrest 79.0\n",
      "russian 79.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 357 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 357, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 357 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[357]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  357\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 357 # cluster 357\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack 764.0\n",
      "london 689.0\n",
      "polic 608.0\n",
      "bridg 266.0\n",
      "terror 194.0\n",
      "kill 149.0\n",
      "saturday 127.0\n",
      "stab 126.0\n",
      "van 119.0\n",
      "british 119.0\n",
      "khan 116.0\n",
      "man 115.0\n",
      "market 112.0\n",
      "manchest 108.0\n",
      "incid 108.0\n",
      "sunday 108.0\n",
      "borough 107.0\n",
      "arrest 104.0\n",
      "butt 102.0\n",
      "june 101.0\n",
      "injur 98.0\n",
      "terrorist 97.0\n",
      "secur 95.0\n",
      "citi 94.0\n",
      "statement 94.0\n",
      "men 91.0\n",
      "night 90.0\n",
      "minist 89.0\n",
      "arm 85.0\n",
      "britain 84.0\n",
      "mayor 83.0\n",
      "tweet 81.0\n",
      "servic 78.0\n",
      "love 76.0\n",
      "dead 75.0\n",
      "shot 75.0\n",
      "twitter 73.0\n",
      "area 72.0\n",
      "prime 72.0\n",
      "metropolitan 71.0\n",
      "islam 71.0\n",
      "critic 70.0\n",
      "respond 68.0\n",
      "restaur 68.0\n",
      "respons 67.0\n",
      "concert 67.0\n",
      "pedestrian 66.0\n",
      "bark 65.0\n",
      "confirm 63.0\n",
      "investig 63.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 937 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 937, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 937 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[937]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  937\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 937 # cluster 937\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la 573.0\n",
      "oscar 375.0\n",
      "land 285.0\n",
      "award 274.0\n",
      "moonlight 247.0\n",
      "film 230.0\n",
      "winner 200.0\n",
      "pictur 194.0\n",
      "win 188.0\n",
      "academi 171.0\n",
      "won 141.0\n",
      "envelop 130.0\n",
      "night 127.0\n",
      "beatti 107.0\n",
      "kimmel 106.0\n",
      "categori 94.0\n",
      "present 94.0\n",
      "actor 92.0\n",
      "dunaway 87.0\n",
      "hollywood 86.0\n",
      "director 85.0\n",
      "actress 83.0\n",
      "movi 82.0\n",
      "stage 80.0\n",
      "produc 78.0\n",
      "nomin 73.0\n",
      "announc 69.0\n",
      "pwc 69.0\n",
      "star 68.0\n",
      "hand 66.0\n",
      "cullinan 66.0\n",
      "sunday 64.0\n",
      "affleck 64.0\n",
      "speech 62.0\n",
      "origin 62.0\n",
      "moment 62.0\n",
      "wrong 60.0\n",
      "stone 60.0\n",
      "happen 59.0\n",
      "read 58.0\n",
      "ceremoni 58.0\n",
      "stori 56.0\n",
      "manchest 56.0\n",
      "jenkin 55.0\n",
      "warren 54.0\n",
      "sea 53.0\n",
      "account 50.0\n",
      "fay 49.0\n",
      "twitter 48.0\n",
      "screenplay 48.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 435 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 435, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 435 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[435]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  435\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 435 # cluster 435\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "griffin 382.0\n",
      "photo 150.0\n",
      "kathi 146.0\n",
      "maher 141.0\n",
      "comedian 95.0\n",
      "twitter 84.0\n",
      "imag 83.0\n",
      "cnn 82.0\n",
      "tuesday 72.0\n",
      "head 72.0\n",
      "apolog 64.0\n",
      "im 62.0\n",
      "statement 59.0\n",
      "photograph 57.0\n",
      "sass 53.0\n",
      "video 53.0\n",
      "night 51.0\n",
      "bill 50.0\n",
      "hold 50.0\n",
      "comment 49.0\n",
      "shield 48.0\n",
      "tweet 46.0\n",
      "joke 44.0\n",
      "eve 42.0\n",
      "friday 42.0\n",
      "bloodi 41.0\n",
      "wrong 40.0\n",
      "barron 40.0\n",
      "shoot 38.0\n",
      "june 38.0\n",
      "blood 36.0\n",
      "sever 36.0\n",
      "son 36.0\n",
      "morn 36.0\n",
      "wednesday 36.0\n",
      "media 34.0\n",
      "word 33.0\n",
      "line 33.0\n",
      "controversi 31.0\n",
      "fire 31.0\n",
      "disgust 30.0\n",
      "come 30.0\n",
      "cooper 29.0\n",
      "children 29.0\n",
      "pose 29.0\n",
      "cross 28.0\n",
      "social 28.0\n",
      "servic 28.0\n",
      "post 27.0\n",
      "tyler 27.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 922 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 922, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 922 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[922]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  922\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 922 # cluster 922\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spicer 330.0\n",
      "inaugur 277.0\n",
      "press 222.0\n",
      "crowd 220.0\n",
      "media 170.0\n",
      "white 135.0\n",
      "fact 120.0\n",
      "claim 110.0\n",
      "size 107.0\n",
      "secretari 97.0\n",
      "saturday 96.0\n",
      "obama 93.0\n",
      "million 89.0\n",
      "washington 78.0\n",
      "sean 68.0\n",
      "conway 67.0\n",
      "estim 67.0\n",
      "fals 63.0\n",
      "lie 60.0\n",
      "administr 59.0\n",
      "cia 58.0\n",
      "friday 57.0\n",
      "mall 57.0\n",
      "statement 57.0\n",
      "brief 56.0\n",
      "januari 53.0\n",
      "audienc 53.0\n",
      "tweet 52.0\n",
      "miller 52.0\n",
      "bust 50.0\n",
      "number 47.0\n",
      "altern 46.0\n",
      "photograph 45.0\n",
      "twitter 44.0\n",
      "largest 41.0\n",
      "sunday 40.0\n",
      "oval 40.0\n",
      "servic 39.0\n",
      "todd 39.0\n",
      "post 37.0\n",
      "falsehood 36.0\n",
      "march 35.0\n",
      "apolog 35.0\n",
      "period 33.0\n",
      "rate 32.0\n",
      "wit 32.0\n",
      "monday 31.0\n",
      "photo 31.0\n",
      "women 30.0\n",
      "cover 30.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# WORD DISTRIBUTION ANALYSIS - CLUSTER 194 (POLITICS)\n",
    "# =============================================================================\n",
    "# Analyze the word distribution for cluster 194, which appears to be the largest cluster.\n",
    "# This cluster seems to contain political news articles based on the top words:\n",
    "# trump, president, state, people, government, etc.\n",
    "\n",
    "# visualize the word distribution of specified cluster:\n",
    "# for cluster 194 (largest cluster - appears to be political news)\n",
    "specified_cls = particle.clusters[194]\n",
    "word_distribution = specified_cls.word_distribution\n",
    "top_num = 50\n",
    "top_word_idx = np.argsort(word_distribution)[::-1][:top_num]\n",
    "for word_id in top_word_idx:\n",
    "    print(id2word[str(word_id)], word_distribution[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: saved JSON/CSV and circular word cloud for cluster:  194\n"
     ]
    }
   ],
   "source": [
    "# 1) Instantiate\n",
    "analyzer = ClusterWordAnalyzer(particle=particle, id2word=id2word)\n",
    "\n",
    "# 2) Frequencies (top 200 by default behavior we set) and save\n",
    "cluster_idx = 194 # cluster 194\n",
    "freqs = analyzer.get_word_freq(cluster_idx=cluster_idx)\n",
    "analyzer.save_word_freq(\n",
    "    freqs,\n",
    "    out_json=f\"results/cluster_{cluster_idx}_wordfreq.json\",\n",
    "    out_csv=f\"results/cluster_{cluster_idx}_wordfreq.csv\"\n",
    ")\n",
    "\n",
    "# 3) Circular word cloud and save\n",
    "analyzer.make_wordcloud(\n",
    "    cluster_idx=cluster_idx,\n",
    "    out_path=f\"results/cluster_{cluster_idx}_wordcloud_circular.png\"\n",
    ")\n",
    "\n",
    "print(\"Done: saved JSON/CSV and circular word cloud for cluster: \", cluster_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 694.6256724921775 avg log-lik per word: -6.543373099736637\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from SMC_sampling import parse_newsitem_2_doc\n",
    "from utils import log_dirichlet_multinomial_distribution\n",
    "\n",
    "def compute_perplexity(particle, test_items, vocabulary_size, theta0):\n",
    "    total_words = 0\n",
    "    log_liks = []\n",
    "    for news_item in test_items:\n",
    "        doc = parse_newsitem_2_doc(news_item, vocabulary_size)\n",
    "        scores = []\n",
    "        for _, cluster in particle.clusters.items():\n",
    "            # aggregate (cluster + doc) just like training code does\n",
    "            cls_word_dist = cluster.word_distribution + doc.word_distribution\n",
    "            cls_word_count = cluster.word_count + doc.word_count\n",
    "            ll = log_dirichlet_multinomial_distribution(\n",
    "                cls_word_dist, doc.word_distribution,\n",
    "                cls_word_count, doc.word_count,\n",
    "                vocabulary_size, theta0\n",
    "            )\n",
    "            scores.append(ll)\n",
    "        log_liks.append(max(scores))\n",
    "        total_words += doc.word_count\n",
    "    avg_log_per_word = sum(log_liks) / total_words\n",
    "    perplexity = float(np.exp(-avg_log_per_word))\n",
    "    return perplexity, avg_log_per_word\n",
    "\n",
    "# Example: load trained particles and evaluate\n",
    "with open('/Users/banosk/Desktop/Pitman-Yor-Hawkes-Process copy/results/particles.pkl','rb') as f:\n",
    "    particles = pickle.load(f)\n",
    "\n",
    "best_particle = max(particles, key=lambda p: p.weight)\n",
    "\n",
    "vocabulary_size = 56720            # use the same as training\n",
    "theta0 = np.array([0.01] * vocabulary_size)  # same prior as training\n",
    "\n",
    "# test_items must be tuples: (doc_id, unix_timestamp, (word_id, count), total_word_count)\n",
    "test_items = [\n",
    "    (123, 1700000000, (789, 3), 100),\n",
    "    # ... add your test docs here ...\n",
    "]\n",
    "\n",
    "ppx, avg_llpw = compute_perplexity(best_particle, test_items, vocabulary_size, theta0)\n",
    "print('perplexity:', ppx, 'avg log-lik per word:', avg_llpw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
